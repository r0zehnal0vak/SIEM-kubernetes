apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: "{{ .Values.namespace }}"
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s # By default, scrape targets every 15 seconds.
      external_labels:
        monitor: 'codelab-monitor'

    # Scrape configuration for Prometheus itself.
    scrape_configs:
      - job_name: 'prometheus'
        scrape_interval: 5s
        static_configs:
          - targets: ['localhost:9090']

      - job_name: 'kubelet-cadvisor'
        scrape_interval: 15s
        metrics_path: /metrics/cadvisor
        scheme: https
        kubernetes_sd_configs:
          - role: node
        tls_config:
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: 'uois'
        scrape_interval: 5s
        static_configs:
          - targets: ['gql_ug:8080']
          - targets: ['gql_forms:8080']
          - targets: ['slo-exporter:8080'] # point to k8s service
          # - metrics_path: ['/prometheus']

      - job_name: 'kube-state-metrics'
        scrape_interval: 15s
        static_configs:
          - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']
          - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8081']

    rule_files:
      - /etc/prometheus/rules-slo.yml
      - /etc/prometheus/watchdog-alerts.yml
      - /etc/prometheus/kube-state-metrics-alerts.yml

  rules-slo.yml: |
    groups:
      - name: ServiceLevelObjectives
        rules:
          - record: service_level_objective:class
            labels: { slo_class: critical }
            expr: 99
          - record: service_level_objective:class
            labels: { slo_class: high-fast }
            expr: 95
          - record: service_level_objective:class
            labels: { slo_class: high-slow }
            expr: 95
          - record: service_level_objective:class
            labels: { slo_class: low }
            expr: 90
          - record: service_level_objective:class
            labels: { slo_class: no-slo }
            expr: 0
      - name: SLO_error_ratio
        rules:
          - record: job:slo_errors_per_request:ratio_rate5m
            expr: |
              sum(rate(slo_domain_slo_class_slo_app:slo_events_total{result="fail"}[5m])) by (slo_app, slo_class, slo_domain, slo_type)
              /
              sum(rate(slo_domain_slo_class_slo_app:slo_events_total[5m])) by (slo_app, slo_class, slo_domain, slo_type)
          - record: job:slo_errors_per_request:ratio_rate30m
            expr: |
              sum(rate(slo_domain_slo_class_slo_app:slo_events_total{result="fail"}[30m])) by (slo_app, slo_class, slo_domain, slo_type)
              /
              sum(rate(slo_domain_slo_class_slo_app:slo_events_total[30m])) by (slo_app, slo_class, slo_domain, slo_type)
  watchdog-alerts.yml: |
    groups:
      - name: watchdog-alerts
        rules:
          - alert: Watchdog
            expr: vector(1)
            labels:
              severity: warning
            annotations:
              description: "This is a Watchdog alert. It's always firing to confirm alerting system health."
              summary: "Watchdog Alert"

  kube-state-metrics-alerts.yml: |
    groups:
      - name: kube-state-metrics
        rules:
        - alert: KubeStateMetricsListErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            summary: kube-state-metrics is experiencing errors in list operations.
          expr: |
            (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
              /
            sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
            > 0.01
          for: 15m
          labels:
            severity: critical
        - alert: KubeStateMetricsWatchErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            summary: kube-state-metrics is experiencing errors in watch operations.
          expr: |
            (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
              /
            sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
            > 0.01
          for: 15m
          labels:
            severity: critical
        - alert: KubeStateMetricsShardingMismatch
          annotations:
            description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
            summary: kube-state-metrics sharding is misconfigured.
          expr: |
            stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) != 0
          for: 15m
          labels:
            severity: critical
        - alert: KubeStateMetricsShardsMissing
          annotations:
            description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
            summary: kube-state-metrics shards are missing.
          expr: |
            2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) - 1
              -
            sum( 2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) ) by (cluster)
            != 0
          for: 15m
          labels:
            severity: critical
        - alert: PodNotRunning
          expr: kube_pod_status_phase{phase!="Running"} > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{`{{ $labels.pod }}`}} is not running!"
            description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} has been in state {{`{{ $labels.phase }}`}} for more than 5 minutes."
        - alert: PodRestartingTooFrequently
          expr: increase(kube_pod_container_status_restarts_total[5m]) > 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{`{{ $labels.pod }}`}} is restarting too frequently!"
            description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} has restarted more than 3 times in the last 5 minutes."
        - alert: DeploymentMissingReplicas
          expr: kube_deployment_status_replicas_available < kube_deployment_spec_replicas
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Deployment {{`{{ $labels.deployment }}`}} has missing replicas!"
            description: "Deployment {{`{{ $labels.deployment }}`}} in namespace {{`{{ $labels.namespace }}`}} has fewer running replicas than desired."

